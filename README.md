# üß† AI-Stack-Starter : Base Architecture for Local AI

![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white)
![NVIDIA](https://img.shields.io/badge/nVIDIA-%2376B900.svg?style=for-the-badge&logo=nvidia&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi)
![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)

Ce d√©p√¥t est un **mod√®le d'architecture (Template)** pour d√©velopper des applications d'Intelligence Artificielle professionnelles en local (On-Premise).

Il met en ≈ìuvre une politique **"Clean Host"** : toute la stack IA est isol√©e dans des conteneurs Docker, avec un acc√®s direct au GPU via le NVIDIA Container Toolkit.

## üèóÔ∏è Architecture Technique

* **Philosophie :** "Clean Host" (Aucune pollution de la machine h√¥te, tout est isol√© dans Docker).
* **H√¥te requis :** Linux (Ubuntu recommand√©) + Drivers NVIDIA uniquement.
* **Virtualisation :** Docker + Docker Compose.
* **Backend IA :** Python 3.10+, FastAPI.
* **Acc√©l√©ration :** CUDA 12.6 + PyTorch (Optimis√© pour RTX 30xx/40xx).

### Structure des dossiers
.
‚îú‚îÄ‚îÄ docker-compose.yml       # Orchestration des services et du GPU
‚îú‚îÄ‚îÄ README.md                # Documentation
‚îî‚îÄ‚îÄ backend-python/          # Microservice IA
    ‚îú‚îÄ‚îÄ Dockerfile           # D√©finition de l'environnement (System layer)
    ‚îú‚îÄ‚îÄ requirements.txt     # D√©pendances Python (App layer)
    ‚îî‚îÄ‚îÄ main.py              # Point d'entr√©e de l'API

## üìã Pr√©-requis (Sur la machine h√¥te)
- Drivers NVIDIA install√©s et fonctionnels (nvidia-smi doit renvoyer un r√©sultat).

- Docker Engine & Docker Compose.

- NVIDIA Container Toolkit configur√©.

## üöÄ Installation & D√©marrage
1. Cloner le projet

2. Lancer la stack
```bash
docker compose up -d --build
```

3. V√©rifier l'acc√®s GPU
```bash
# Via le terminal
curl http://localhost:5000/gpu-check

# Ou via le navigateur
# http://localhost:5000/gpu-check
```


R√©ponse attendue :
```json
{
    "cuda_available": true,
    "device_count": 1,
    "current_device": "NVIDIA GeForce RTX 4090",
    "cuda_version_torch": "12.6.1",
    "driver_version": "535.124.06"
}
``` 

## üîß Personnalisation

### Ajouter une librairie Python
Ajouter la ligne dans backend-python/requirements.txt.

Relancer avec docker compose up -d --build.

### Changer de port
Si le port 5000 est occup√© sur votre machine, modifiez le fichier docker-compose.yml :
```yaml
ports:
  - "NOUVEAU_PORT:8000"
```



