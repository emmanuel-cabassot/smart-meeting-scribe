from fastapi import FastAPI, UploadFile, File, HTTPException
import whisper
import torch
import shutil
import os

app = FastAPI(title="Smart Meeting Scribe")

# Variable globale pour savoir quel mod√®le est r√©ellement charg√©
LOADED_MODEL_NAME = "Inconnu"

# --- 1. CONFIGURATION MAT√âRIELLE (Au d√©marrage) ---
print("‚è≥ Initialisation du syst√®me...")

# On v√©rifie si la RTX est bien l√†
if torch.cuda.is_available():
    DEVICE = "cuda"
    GPU_NAME = torch.cuda.get_device_name(0)
    print(f"üöÄ GPU D√©tect√© : {GPU_NAME}")
else:
    DEVICE = "cpu"
    print("‚ö†Ô∏è GPU non d√©tect√©, passage en mode CPU (Lent).")

# --- 2. CHARGEMENT DU MOD√àLE (Une seule fois !) ---
# On charge le mod√®le au niveau global pour qu'il reste en m√©moire RAM/VRAM
try:
    print(f"‚è≥ Tentative de chargement du mod√®le Whisper TURBO sur {DEVICE}...")
    model = whisper.load_model("turbo", device=DEVICE)
    LOADED_MODEL_NAME = "turbo"
    print("‚úÖ Mod√®le TURBO charg√© et pr√™t !")

except Exception as e:
    print(f"‚ö†Ô∏è Le mod√®le 'turbo' n'a pas pu √™tre charg√© (Erreur: {e})")
    print("üîÑ Bascule automatique sur le mod√®le 'medium' (Valeur s√ªre)...")
    
    try:
        # Fallback : Medium est un excellent compromis pour une RTX 30xx/40xx
        model = whisper.load_model("medium", device=DEVICE)
        LOADED_MODEL_NAME = "medium"
        print("‚úÖ Mod√®le MEDIUM charg√© (Mode de secours activ√©) !")
    except Exception as e2:
        print(f"‚ùå Erreur critique : Impossible de charger un mod√®le. {e2}")
        raise e2


# --- 3. LES ROUTES API ---

@app.get("/")
def read_root():
    """Route de sant√© pour v√©rifier que l'API tourne"""
    return {
        "status": "Smart Meeting Scribe Ready", 
        "device": DEVICE,
        "model_loaded": LOADED_MODEL_NAME
    }

@app.get("/gpu-check")
def check_gpu():
    """V√©rifie l'√©tat de la carte graphique et de la m√©moire"""
    try:
        gpu_stats = {
            "available": torch.cuda.is_available(),
            "device_count": torch.cuda.device_count(),
            "current_device": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU",
            "active_model": LOADED_MODEL_NAME
        }
        return gpu_stats
    except Exception as e:
        return {"error": str(e)}

@app.post("/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    """
    Endpoint principal : Re√ßoit un fichier audio -> Renvoie le texte.
    """
    
    # 1. Sauvegarde temporaire du fichier re√ßu
    # On nettoie le nom de fichier pour √©viter les bugs d'accents/espaces
    clean_name = "".join(x for x in file.filename if x.isalnum() or x in "._-")
    temp_filename = f"temp_{clean_name}"
    
    try:
        with open(temp_filename, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # 2. Transcription
        print(f"üéôÔ∏è Traitement de {file.filename} avec le mod√®le {LOADED_MODEL_NAME}...")
        
        # L'appel magique √† Whisper
        # Tu peux ajouter initial_prompt="Compte rendu de r√©union" pour aider l'IA
        result = model.transcribe(temp_filename)
        
        print("‚úÖ Transcription termin√©e.")
        
        return {
            "filename": file.filename,
            "language_detected": result["language"],
            "text": result["text"].strip()
        }

    except Exception as e:
        print(f"‚ùå Erreur pendant la transcription : {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
    finally:
        # 3. Nettoyage (Toujours supprimer le fichier temp, m√™me si √ßa plante)
        if os.path.exists(temp_filename):
            os.remove(temp_filename)
            print(f"üßπ Fichier temporaire supprim√©.")