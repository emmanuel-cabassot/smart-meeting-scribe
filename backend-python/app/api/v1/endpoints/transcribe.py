"""
Endpoint de transcription audio.
Orchestration du pipeline : Audio ‚Üí Diarisation ‚Üí Identification ‚Üí Transcription ‚Üí Fusion
"""

from fastapi import APIRouter, UploadFile, File, HTTPException
import shutil
import time
import traceback
import os

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# IMPORTS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
from app.core.models import release_models, load_embedding_model
from app.services.audio import convert_to_wav, cleanup_files
from app.services.diarization import run_diarization
from app.services.transcription import run_transcription
from app.services.identification import get_voice_bank_embeddings, identify_speaker
from app.services.fusion import merge_transcription_diarization
from app.services.storage import save_results

router = APIRouter()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# UTILITAIRE DE LOGGING
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
def log_step(message, start_time=None):
    """Affiche un log visuel avec la dur√©e si start_time est fourni."""
    if start_time:
        duration = time.time() - start_time
        print(f"   ‚úÖ Termin√© en {duration:.2f} secondes.")
        print(f"---------------------------------------------------")
    else:
        print(f"\nüöÄ [√âTAPE] {message}")
        print(f"---------------------------------------------------")
    return time.time()

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# ROUTE PRINCIPALE
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
@router.post("/")
async def transcribe_audio(file: UploadFile = File(...)):
    """
    Orchestre tout le processus IA de A √† Z.
    """
    # Timer Global
    global_start = time.time()
    
    clean_name = "".join(x for x in file.filename if x.isalnum() or x in "._-")
    temp_filename = f"temp_{clean_name}"
    wav_filename = None 
    
    print(f"\n\n===================================================")
    print(f"üì• R√âCEPTION : {clean_name}")
    print(f"===================================================")

    try:
        # --- √âTAPE 0 : Sauvegarde ---
        t0 = log_step("0. Sauvegarde Fichier Temporaire")
        with open(temp_filename, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        log_step("Fichier sauvegard√©", t0)
        
        # --- √âTAPE 1 : Conversion ---
        t1 = log_step("1. Conversion Audio (WAV 16kHz Mono)")
        wav_filename = convert_to_wav(temp_filename)
        log_step("Conversion termin√©e", t1)
        
        # --- √âTAPE 2 : Diarisation ---
        t2 = log_step("2. Diarisation (Qui parle quand ?)")
        annotation = run_diarization(wav_filename)
        
        # Petit log stat
        nb_segments = len(list(annotation.itertracks(yield_label=True)))
        print(f"   üìä Segments d√©tect√©s : {nb_segments}")
        
        release_models() # Important : On vide pour laisser la place √† la suite
        log_step("Diarisation termin√©e", t2)
        
        # --- √âTAPE 3 : Identification ---
        t3 = log_step("3. Identification Vocale (WeSpeaker)")
        bank_embeddings = get_voice_bank_embeddings()
        speaker_mapping = {}
        
        if bank_embeddings:
            print(f"   üìÇ Banque de voix charg√©e ({len(bank_embeddings)} profils)")
            emb_model = load_embedding_model()
            
            # On r√©cup√®re les labels (SPEAKER_00, SPEAKER_01...)
            detected_labels = annotation.labels()
            
            for label in detected_labels:
                # On cherche le segment le plus long pour ce speaker pour avoir une bonne identification
                track_segment = next((s for s, _, l in annotation.itertracks(yield_label=True) if l == label and s.duration > 2.0), None)
                
                if track_segment:
                    # On d√©coupe l'audio sur ce segment pr√©cis
                    unknown_emb = emb_model.crop(wav_filename, track_segment)
                    # On compare
                    name, score = identify_speaker(unknown_emb, bank_embeddings)
                    
                    if name:
                        print(f"   üîç {label} identifi√© comme üë§ {name} (Score: {score:.2f})")
                        speaker_mapping[label] = name
                    else:
                        print(f"   ‚ùì {label} inconnu")
                        speaker_mapping[label] = label
                else:
                    speaker_mapping[label] = label
        else:
            print("   ‚ö†Ô∏è Pas de banque de voix trouv√©e, identification saut√©e.")
            
        release_models()
        log_step("Identification termin√©e", t3)
        
        # --- √âTAPE 4 : Transcription ---
        t4 = log_step("4. Transcription (Whisper Large-v3)")
        segments = run_transcription(wav_filename)
        print(f"   üìù Phrases transcrites : {len(segments)}")
        release_models()
        log_step("Transcription termin√©e", t4)
        
        # --- √âTAPE 5 : Fusion & Sauvegarde ---
        t5 = log_step("5. Fusion & R√©sultat Final")
        
        # On applique le mapping des noms (SPEAKER_00 -> Emmanuel)
        final_result = merge_transcription_diarization(segments, annotation)
        for item in final_result:
            if item["speaker"] in speaker_mapping:
                item["speaker"] = speaker_mapping[item["speaker"]]
        
        save_path = save_results(clean_name, annotation, segments, final_result)
        
        log_step(f"Sauvegarde effectu√©e dans : {save_path}", t5)
        
        # Bilan
        total_duration = time.time() - global_start
        print(f"\n‚ú® SUCC√àS - Dur√©e totale : {total_duration:.2f}s ‚ú®")
        print(f"===================================================\n")

        return {
            "metadata": {
                "filename": clean_name,
                "duration_process": round(total_duration, 2),
                "saved_at": save_path
            }, 
            "segments": final_result
        }

    except Exception as e:
        print(f"\n‚ùå ERREUR CRITIQUE : {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
        
    finally:
        if wav_filename:
            cleanup_files(temp_filename, wav_filename)
        else:
            cleanup_files(temp_filename)
        
        release_models()
