"""
Gestionnaire de modÃ¨les IA (Cycle de vie & VRAM).
"""
from faster_whisper import WhisperModel
from pyannote.audio import Pipeline, Model, Inference
import torch
import gc
from app.core.config import DEVICE, COMPUTE_TYPE, HF_TOKEN

# Variables globales (Singletons)
current_whisper = None
current_pipeline = None
current_embedding = None

# ID SpÃ©cifique pour Whisper Turbo optimisÃ© CTranslate2 (Gain VRAM ~1.5GB)
WHISPER_MODEL_ID = "deepdml/faster-whisper-large-v3-turbo-ct2"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# UTILITAIRE VRAM
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def log_vram(action: str, model_name: str):
    """Affiche l'Ã©tat de la mÃ©moire GPU."""
    if torch.cuda.is_available():
        free_mem, total_mem = torch.cuda.mem_get_info()
        free_gb = free_mem / 1024**3
        total_gb = total_mem / 1024**3
        used_gb = total_gb - free_gb
        print(f"   ğŸ’¾ [VRAM] {action} {model_name} | OccupÃ©: {used_gb:.2f}GB / {total_gb:.2f}GB (Libre: {free_gb:.2f}GB)")
    else:
        print(f"   ğŸ’» [CPU] {action} {model_name}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CHARGEMENT DES MODÃˆLES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def load_whisper():
    global current_whisper
    if current_whisper is None:
        print(f"   â³ Initialisation du chargement de Whisper Turbo ({WHISPER_MODEL_ID}) en {COMPUTE_TYPE}...")
        # On charge le modÃ¨le Turbo optimisÃ© (CTranslate2)
        # Note : compute_type="int8" est recommandÃ© pour maximiser le gain VRAM sur la RTX 4070
        current_whisper = WhisperModel(
            WHISPER_MODEL_ID, 
            device=DEVICE, 
            compute_type=COMPUTE_TYPE
        )
        # On loggue l'Ã©tat APRÃˆS le chargement pour voir le poids rÃ©el
        log_vram("âœ… ModÃ¨le ChargÃ© :", "Whisper Large-v3-Turbo")
    return current_whisper

def load_pyannote():
    """Charge Pyannote avec un patch de sÃ©curitÃ© compatible PyTorch 2.6."""
    global current_pipeline
    if current_pipeline is None:
        print("   â³ Initialisation du chargement de Pyannote (Segmentation)...")
        
        # Patch sÃ©curitÃ© pour torch.load
        original_load = torch.load
        def robust_load(*args, **kwargs):
            kwargs["weights_only"] = False
            return original_load(*args, **kwargs)
        torch.load = robust_load
        
        try:
            current_pipeline = Pipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1", 
                token=HF_TOKEN
            )
            current_pipeline.to(torch.device(DEVICE))
            # On loggue l'Ã©tat APRÃˆS l'envoi sur le GPU
            log_vram("âœ… ModÃ¨le ChargÃ© :", "Pyannote Diarization")
        finally:
            torch.load = original_load 
            
    return current_pipeline

def load_embedding_model():
    """Charge WeSpeaker (Natif Pyannote)."""
    global current_embedding
    if current_embedding is None:
        print("   â³ Initialisation du chargement de WeSpeaker (Identification)...")
        
        original_load = torch.load
        def robust_load(*args, **kwargs):
            kwargs["weights_only"] = False
            return original_load(*args, **kwargs)
        torch.load = robust_load
        
        try:
            model = Model.from_pretrained(
                "pyannote/wespeaker-voxceleb-resnet34-LM", 
                token=HF_TOKEN
            )
            current_embedding = Inference(model, window="whole")
            current_embedding.to(torch.device(DEVICE))
            # On loggue l'Ã©tat APRÃˆS l'envoi sur le GPU
            log_vram("âœ… ModÃ¨le ChargÃ© :", "WeSpeaker (ResNet34)")
        finally:
            torch.load = original_load
            
    return current_embedding

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NETTOYAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def release_models():
    """Vide la VRAM proprement et loggue ce qui a Ã©tÃ© libÃ©rÃ©."""
    global current_whisper, current_pipeline, current_embedding
    
    freed_models = []
    
    if current_whisper is not None:
        del current_whisper
        current_whisper = None
        freed_models.append("Whisper Turbo")
    
    if current_pipeline is not None:
        del current_pipeline
        current_pipeline = None
        freed_models.append("Pyannote")
    
    if current_embedding is not None:
        del current_embedding
        current_embedding = None
        freed_models.append("WeSpeaker")
    
    # Garbage Collection
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
    
    # Log si quelque chose a Ã©tÃ© libÃ©rÃ©
    if freed_models:
        print(f"   ğŸ§¹ [NETTOYAGE] ModÃ¨les dÃ©chargÃ©s : {', '.join(freed_models)}")
        if torch.cuda.is_available():
            free_mem, total_mem = torch.cuda.mem_get_info()
            # On affiche combien on a rÃ©cupÃ©rÃ©
            print(f"        â†³ VRAM Libre maintenant : {free_mem / 1024**3:.2f} GB / {total_mem / 1024**3:.2f} GB")